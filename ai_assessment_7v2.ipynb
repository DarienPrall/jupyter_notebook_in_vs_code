{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Sentiment Analysis (Text Classification) Using IMDB Moview Reviews Dataset**\n",
    "\n",
    "**Objective**\n",
    "In this notebook, I will be utilizing the IMDB moview reviews dataset to perform Sentiment Analysis (Text Classification) using Python. \n",
    "\n",
    "**Steps**\n",
    "1. Define assets\n",
    "2. Import necessary libraries\n",
    "3. Understanding Text Processing (Stop Words)\n",
    "4. Loading and Processing the IMDB dataset\n",
    "5. Split into Training and Testing Sets\n",
    "6. Convert text into numerical features\n",
    "7. Training the Model\n",
    "8. Evaluate the Model\n",
    "9. Visualize Model Performance\n",
    "10. Test Model with New Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Step 1. Define assets**\n",
    "\n",
    "###**Link**\n",
    "For this project, I will need to download the IMDB moview reviews dataset. \n",
    "Link Here --> [IMDB](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "###**Citation**\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "\n",
    "###**Folders**\n",
    "There are two top-level directories [train/, test/] corresponding to\n",
    "the training and test sets. Each contains [pos/, neg/] directories for\n",
    "the reviews with binary labels positive and negative. Within these\n",
    "directories, reviews are stored in text files named following the\n",
    "convention id_rating.txt where id is a unique id and rating is\n",
    "the star rating for that review on a 1-10 scale.\n",
    "\n",
    "These folders both contain over 12k files. I will reduce it down to 50 files each for speed purposes.\n",
    "- test/\n",
    "    - neg/\n",
    "    - pos/\n",
    "- train/\n",
    "    - neg/\n",
    "    - pos/\n",
    "    - unsup/\n",
    "\n",
    "I will be using the train/pos/ and train/neg/ for training and the test/pos/ and test/neg/ for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Step 2. Import Necessary Libraries**\n",
    "The libraries I will be using as as follows:\n",
    "\n",
    "- nltk\n",
    "- scikit-learn\n",
    "    - model_selection (train_test_split)\n",
    "    - feature_extraction.text (TfidVectorizer)\n",
    "    - naive_bayes (MultinominalNB)\n",
    "    - metrics (classification_report, accuracy_score)\n",
    "- matplotlib\n",
    "\n",
    "I will also be accessing my OS to access the dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Step 3: Understanding Text Processing (Stopwords)**\n",
    "There are words in text processing that have almost no value in information by themselves such as: \"and\", \"to\", \"is\", etc..\n",
    "By not focusing on these words, it allows for less noise in the dataset and allows the model to focus on the ones with more value. \n",
    "The Natural Language Toolkit (NLK) already defines these stopwords so you don't have to manually enter all of them. \n",
    "We can simply download the stopwords with the following Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Step 4: Loading and Processing the IMDB dataset**\n",
    "The two training files I'm working with are:\n",
    "- train/pos/\n",
    "- train/neg/\n",
    "\n",
    "These folders need to be defined in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_directory = ('/Users/darienprall/Documents/GitHub/aclImdb/train/pos')\n",
    "train_neg_directory = ('/Users/darienprall/Documents/GitHub/aclImdb/train/neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I need to store all files from each the positive and negative reviews to an array. I can do this by first, listing all of the names of the files in the directory. I then need to obtain the full directory path for each file by joining the directory and the filename. Once I have all the full path names, I can open each file, using the full path name, and add its contents to the array.\n",
    "\n",
    "Seeing that the large dataset contains over 48,000 files in total, I will have it only look at the first 500 files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated run time 39mins, reduced file count for faster run time\n",
    "positive_reviews = [\n",
    "    open(os.path.join(train_pos_directory, f)).read() for f in os.listdir(train_pos_directory)[:500]\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    open(os.path.join(train_neg_directory, f)).read() for f in os.listdir(train_neg_directory)[:500]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the two lists split, I can join them into one big list using concatenation so that it contains all text reviews, the first part of the list will be postiive reveiews, and the second part is negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = positive_reviews + negative_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all reviews stored, I need to do the following:\n",
    "- Create a list of labels for each review as positive (1) or negative (0)\n",
    "- Make sure theres randomness by shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of labels for each review as positive (1) or negative (0)\n",
    "labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
    "\n",
    "# Ensure Randomness\n",
    "data = list(zip(texts, labels))\n",
    "random.shuffle(data)\n",
    "texts, lables = zip(*data)\n",
    "\n",
    "#print(texts[:2])\n",
    "#print(labels[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Step 5: Split Into Testing and Training Sets**\n",
    "Using the train_test_split from sklearn, I can set the test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(f\"Training data size: {len(X_train)}\")\n",
    "print(f\"Test data size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Step 6: Convert Text to Numerical Values**\n",
    "Machine learning needs numerical features. Do do this, I will have to use TF-IDF vectorization to convert the text into a numeric form. Then, I can fit and transform the training data, as well as transform the testing data using the same vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 file run time average 5mins\n",
    "# 500 file run time average 39 mins\n",
    "# 5000 file run time \n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 2000)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Training data shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Testing data shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Step 7: Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Step 8: Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_prediction)\n",
    "print(f\"The accuracy score is: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report: \")\n",
    "print(classification_report(y_test, y_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Classification Report Analysis**\n",
    "\n",
    "###Precision Score\n",
    "The classification report shows the precision score predicted 49% of the negative reviews correctly and 46% of the positive reviews correctly. This is not a well performing model based on these results. \n",
    "\n",
    "I have increased the total file input from 50 > 100 > 500. But the percentages stay the around the same percentages. I may have to do more than 5000 files to see any significant change but its not gauranteed to increase the accuracy. \n",
    "\n",
    "The other reason for the low precision rates could be that the Naive Bayers model might not be the best fit for this dataset. \n",
    "\n",
    "The model was better a predicting postive reviews rather than negative reviews correctly. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
